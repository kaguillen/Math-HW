\documentclass[12pt]{article}
%------------------------------- BEGIN PREAMBLE
% packages used
\usepackage{amssymb,amsmath,amsfonts,mathrsfs,pgffor,marvosym,amsthm,mathtools}
% macros
\DeclarePairedDelimiter\set\{\}
\newcommand      {\Nm}         {{\mathbb N}}
\newcommand      {\Zm}         {{\mathbb Z}}
\newcommand      {\Qm}         {{\mathbb Q}}
\newcommand      {\Rm}         {{\mathbb R}}
\newcommand      {\Cm}         {{\mathbb C}}
\newcommand      {\vb}        {\mathbf}
\newcommand      {\PP}        {{\mathscr P}}
\newcommand      {\Fm}          {{\mathbb F}}
\newcommand {\lines}[1] {\foreach \n in {1,...,#1}{ \vspace{9mm} \hrule height 
0.2pt  }\vspace{2mm} }

\newcommand {\w} {{\wedge}}
\newcommand {\e}[3]{{#1e_1 + #2e_2 + #3e_3}}
\newcommand {\f}[1]{{#1_1 dx + #1_2 dy + #1_3 dz}}
\newcommand {\ff}[2]{{(#1_1 + #2_1) dx + (#1_2+ #2_2) dy + (#1_3 +#2_3)dz}}
\newcommand {\fff}[3]{{(#1_1 + #2_1 + #3_1) dx + (#1_2+ #2_2 + #3_2) dy + (#1_3 +#2_3 + #3_3)dz}}
% adjustment of page dimensions
\textwidth=7in
\textheight=9.8in
\topmargin= -0.8in
\oddsidemargin= -0.5in
\evensidemargin= 0.0in
\setlength{\parskip}{1ex plus0.5ex minus0.2ex}
\setlength{\jot}{10pt}
%-------------------------------- END PREAMBLE
\begin{document}
\begin{flushright}
    Name: Kevin Guillen \\*
    Student ID: 1747199
\end{flushright}
\begin{center}
    {\bf 117 - SS2 - HW3 - August 25th, 2021}
\end{center}

\begin{itemize}
%------------------------------------------------------PROBLEM 1---------------------------------------------------------
    \item[$\textbf{[1]}$]
    Let $V$ and $W$ be finite-dimensional $\mathbb{F}$-vector spaces.
    \begin{itemize}
    
    \vspace{.3cm}
    \item[(a)]
    Show that $\dim(\text{Hom}(V,W)) = \dim(V)\dim(W)$ by finding an explicit basis.

    \begin{proof}
        Since $V$ and $W$ are both finite, let the dimension of $V$ and the dimension of $W$ be denoted by $n$ and $m$ respectively. By definition that means the basis for $V$ and $W$ are the following.
        \begin{align*}
            \mathcal{B_V} &= \set{v_1, v_2, \dots, v_n} \\
            \mathcal{B_W} &= \set{w_1, w_2, \dots, w_m}
        \end{align*} 

        Now let us define the linear maps $\pi_{ij}: V \to W$ for $1\leq i \leq n$ and $1 \leq j \leq m$ by the following,
        \begin{align*}
            \pi_{ij}(v_p)= \begin{cases}w_j & p = i \\ 0 & p\neq i \end{cases}
        \end{align*}

        These will serve as a basis for Hom$(V,W)$, and we will prove it with the following. Let $\alpha_{ij}$ be a scalar and assume we have,
        \begin{align*}
            \pi = \sum_{i,j}^{n,m} \alpha_{ij} \pi_{ij} = 0 
        \end{align*}
        This would mean for $\pi(v_i)$ and $i \in \set{1, 2, \dots, n}$,
        \begin{align*}
            \pi(v_i) = \sum_{j}^m \alpha_{ij} w_j = 0 
        \end{align*}
        Recall though that the set of vector $w_j$ for $1 \leq j \leq m$ are linearly independent, and thus our maps $\pi_{ij}$ are also linearly independent. 

        Now take any function $\pi$ from Hom$(V,W)$. We can define it its values when inputting the basis of $V$ as $\pi(v_i)\in W$. Meaning when $i \in {1,2,\dots, n}$ and $\alpha_{ij}$ as a scalar, we can express $\pi(v_i)$ as,
        \begin{align*}
            \pi(v_i) = \sum_{j}^m \alpha_{ij}w_j
        \end{align*} 
        Which means,
        \begin{align*}
            \pi = \sum_{i,j}^{n,m} \alpha_{ij}\pi_{ij}
        \end{align*}
        because the linear functions agree on basis vectors. This means for $1 \leq i \leq n$ and $1 \leq j \leq m$,
        \[\text{Hom}(V,W) = \text{span}(\set{\pi_{ij}})\]

        This is the proof since we know there are $\text{dim}(V)\text{dim}(W)$ of these functions. 

    \end{proof}
    
    \vspace{.3cm}
    \item[(b)]
    Show that $\text{Hom}(V,V) \cong V \otimes V^*$.
    Sorry if this isn't formal/rigorous enough I saved this for last since I get the reasoning, but I'm running out of time. 
    \begin{proof}
        From class (prop 9.1) we showed that setting $U =V$ for Hom$(U,V)$ that $V^* \otimes V \cong \text{End}(V)$ and that really End$(V)$ is just Hom$(V,V)$. 
        
        We also know from class and a previous homework (or maybe I think a mastery problem) that $V^*\otimes V \cong V \otimes V^*$. Thus through transitivity we can compose these isomorphisms and we'll end up with $\text{Hom}(V,V) \cong V \otimes V^*$.
    \end{proof}
    
    \end{itemize}
    
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 2---------------------------------------------------------   
    \item[$\textbf{[2]}$]
    Let $T: \mathbb{F}^3 \rightarrow \mathbb{F}^3$ be the linear transformation with matrix:
    \begin{equation*}
    \mathbf{A} = \begin{pmatrix}
    1 & 4 & 3 \\
    3 & 4 & 1 \\
    4 & 4 & 4
    \end{pmatrix}
    \end{equation*}
    Compute the standard matrix $[\Lambda^2 T]$ with respect to the standard basis $\{e_1 \wedge e_2, e_1 \wedge e_3, e_2 \wedge e_3\}$ of $\Lambda^2(\mathbb{F}^3)$. 

    \textbf{Solution:} First let's get the computations for $T(e_i)$ out of the way,
        \begin{align*}
            T(e_1) = \begin{pmatrix}
                1 & 4 & 3 \\ 3 & 4 & 1 \\ 4 & 4 & 4
            \end{pmatrix} \begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix} = \begin{pmatrix}
                1 \\ 3 \\ 4
            \end{pmatrix} \\ 
            T(e_2) = \begin{pmatrix}
                1 & 4 & 3 \\ 3 & 4 & 1 \\ 4 & 4 & 4
            \end{pmatrix} \begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix} = \begin{pmatrix}
                4\\ 4 \\ 4
            \end{pmatrix} \\
            T(e_3) = \begin{pmatrix}
                1 & 4 & 3 \\ 3 & 4 & 1 \\ 4 & 4 & 4
            \end{pmatrix} \begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix} = \begin{pmatrix}
                3 \\ 1 \\ 4
            \end{pmatrix}
        \end{align*}
    Then we solving for the standard matrix with respect to the standard basis we get,
    \begin{align*}
        T(e_1 \wedge e_2) &= T(e_1) \wedge T(e_2) \\
        &=(\e{1}{3}{4}) \w (\e{4}{4}{4}) \\
        &= (4-12)e_1 \w e_2 + (4-16) e_1 \w e_3 + (12-16) e_2 \w e_3 \\ 
        &= -8(e_1 \w e_2) -12(e_1 \w e_3) -4(e_2\w e_3)\\
        T(e_1 \w e_3) &= T(e_1) \w T(e_3) \\ 
        &= (\e{1}{3}{4}) \w (\e{3}{1}{4}) \\
        &= (1-9)e_1 \w e_2 + (4-12)e_1 \w e_3 + (12 - 4)e_2 \w e_3 \\
        &= -8(e_1 \w e_2) -8(e_1 \w e_3) + 8 (e_2 \w e_3) \\
        T(e_2 \w e_3) &= T(e_2) \w T(e_3) \\
        &= (\e{4}{4}{4}) \w (\e{3}{1}{4}) \\
        &= (4 - 12)e_1 \w e_2 + (16-12)e_1\w e_3 + (16-4)e_2 \w e_3 \\
        &= -8(e_1 \w e_2) +4 (e_1 \w e_3) + 12(e_2 \w e_3)
    \end{align*}
    Now like in example 12.4 we can read off our coefficients to get the standard matrix and we get the following,
    \[\begin{pmatrix}
        -8 & -8 & -8 \\ -12 & -8 & 4 \\ -4 & 8 & 12
    \end{pmatrix}\]
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 3---------------------------------------------------------   
    \item[$\textbf{[3]}$]
    Let $V$ be a $\mathbb{F}$-vector space. Show that if $T,S \in \text{End}(V)$ such that $ST - TS$ commutes with $S$, then for every $k \in \mathbb{N}$:
    \begin{equation*}
    S^kT - TS^k = kS^{k-1}(ST - TS)
    \end{equation*} 

    \begin{proof}
        Base case where $k = 1$
        \[S^1T - TS^1 = 1S^0(S^1T -TS^1)\]
        we see is true.

        Now assume it holds for $k = n$

        Now for $k = n +1$
        \begin{align*}
            S^{n+1}T - TS^{n+1} \\
            S^nST - TS^nS && \text{Recall though $ST-TS$ commutes with $S$} \\
            (n+1)S^n(ST-TS)
        \end{align*}

        We know it holds for $k = n$ and thus by induction it holds for $k = n+1$
    \end{proof}
    
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 4--------------------------------------------------------- 
    \item[$\textbf{[4]}$]
    Let $V$ be a $\mathbb{F}$-vector space. Show that if $T \in \text{End}(V)$ such that $T^2 - T + I = 0$, then $T$ is invertible. 
    \begin{proof}
        \begin{align*}
            T^2 - T + I &= 0 \\ 
            T^2 &= T-I && I = TT^{-1} \\
            T^2 &= T-TT^{-1} \\
            T^2 &= T(I - T^{-1}) \\
            T &= (I - T^{-1})
        \end{align*}

        Therefore $T$ is invertible. 
    \end{proof}
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 5---------------------------------------------------------
    \item[$\textbf{[5]}$]
    Let $V$ be a $\mathbb{F}$-vector space. If $S,T \in \text{End}(V)$ such that $ST = 0$, does it follow that $TS = 0$?
    \begin{proof}
        Consider the vector space $\Rm^2$ over $\Rm$. We have in End$(V)$ the following,
        \begin{align*}
            S &= \begin{pmatrix}
                1 & -1 \\ 1 & -1
            \end{pmatrix} \\
            T &= \begin{pmatrix}
                1 & 1 \\ 1 & 1
            \end{pmatrix}
        \end{align*}

        We see though that,
        \begin{align*}
            ST = \begin{pmatrix}
                1 & -1 \\ 1 & -1
            \end{pmatrix}\begin{pmatrix}
                1 & 1 \\ 1 & 1
            \end{pmatrix} = \begin{pmatrix}
                0 & 0 \\ 0 & 0
            \end{pmatrix} = 0
        \end{align*}
        but,
        \begin{align*}
            TS = \begin{pmatrix}
                1 & 1 \\ 1 & 1
            \end{pmatrix}\begin{pmatrix}
                1 & -1 \\ 1 & -1
            \end{pmatrix} = \begin{pmatrix}
                2 & -2 \\ 2 & -2
            \end{pmatrix} \neq ST
        \end{align*}

        So, no. If we have two linear transformation $S$ and $T$ such that $ST = 0$ it does not follow that $TS = 0$
    \end{proof}
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 6---------------------------------------------------------  
    \item[$\textbf{[6]}$]
    Let $\mathbb{P}_n[x]$ denote the $\mathbb{F}$-vector space of all polynomials with degree less than or equal to $n$ whose coefficients come from $\mathbb{F}$. Suppose that $L \in \text{End}(V)$ such that $Lp(x) = p(x + 1)$ for every $p(x) \in \mathbb{P}_n[x]$. Prove that if $D$ is the differentiation operator defined through the power rule, then:
    \begin{equation*}
    I + \frac{D}{1!} + \frac{D^2}{2!} + \dots + \frac{D^{n-1}}{(n - 1)!} = L
    \end{equation*}

    \begin{proof}
        We are given that $Lp(x) = p(x+1)$. We know $p(x) \in \mathbb{P}_n[x]$ is of the form
        \[p(x) = a_nx^n + \dots + a_1x + a_o\]
        This means $Dp(x)$, where $D$ is the differentiation operator, is,
        \begin{align*}
            \dfrac{d}{dx}p(x) = na_nx^{n-1} + \dots + 2a_2x + a_1 + 0
        \end{align*}

        We see for different powers of $D$,
        \begin{align*}
            \dfrac{d^2}{dx^2}p(x) &= n(n-1)a_nx^{n-2} + \dots + 6a_3x + 2a_2 + 0 \\
            \dfrac{d^3}{dx^3}(x) &= n(n-1)(n-2)x^{n-3} + \dots + 24a_4x + 6a_3 + 0 \\
            &\vdots \\
            \dfrac{d^{n-1}}{dx^{n-1}}p(x) &= n\cdot(n-1)\cdot(n-2)\dots 2\cdot a_n x + 0 = n!a_nx
        \end{align*}

        Now consider $\left(I + \frac{D}{1!} + \frac{D^2}{2!} + \dots + \frac{D^{n-1}}{(n - 1)!}\right)p(x)$ 
        \begin{align*}
            &= p(x) + \frac{D}{1!}p(x) + \frac{D^2}{2!}p(x)   + \dots + \dfrac{D^{n-1}}{(n-1)!}p(x) + \dfrac{D^n}{n!}p(x) \\
            &= (a_nx^n + \dots + a_1x + a_0) + (na_nx^{n-1} + \dots + 2a_2x + a_1)  \\
            &+ (\frac{n(n-1)}{2}a_nx^{n-2} + \dots + 3a_3x + a_2)  + (\frac{n(n-1)(n-2)}{6}x^{n-3} + \dots + 4a_4x + a_3 + 0) \\
            &+ \dots + na_nx + a_n \\
            &= a_n(x^n + nx^{n+1} + nx^{n-1} + \frac{n(n-1)}{2}x^{n-2} + \dots + nx + 1) + \dots + a_3(x^3 + 3x^2 + 3x + 1) \\
            &+ a_2(x^2 + 2x + 1) + a_1(x+1) + a_0 \\
            &= a_n(x+1)^n + \dots + a_3(x+1)^3 + a_2(x+1)^2 + a_1(x+1) + a_0 \\
            &= p(x+1) \\
            &= Lp(x)
        \end{align*}

        Now if $p(x)$ were to be deg$(p(x)) < n$ say for this case specifically $n-1$ through the same method as shown we'd see $D^{n-1}p(x) = (n-1)!a_{n-1}$

        And we see 
        \begin{align*}
            \left(I + \frac{D}{1!} + \frac{D^2}{2!} + \dots + \frac{D^{n-1}}{(n - 1)!}\right)p(x) &= a_{n-1}(x^{n-1} + (n-1)x^{n-2} + \dots + 1) \\ &+ a_2(x^2 + 2x + 1) + a_1(x+1) + a_0 \\
            &= a_{n-1}(x+1)^{n-1} + \dots + a_2(x+1)^2 + a_1(x+1) + a_0 \\
           &= p(x+1) \\
           &= Lp(x) 
        \end{align*}

        therefore we have as desired that $\left(I + \frac{D}{1!} + \frac{D^2}{2!} + \dots + \frac{D^{n-1}}{(n - 1)!}\right)p(x) = L$
        \end{proof}
    
    \vspace{.5cm}
%-------------------------------------------------PROBLEM 7---------------------------------------------------------   
    \item[$\textbf{[7]}$]
    Let $V$ be a $\mathbb{F}$-vector space with subspaces $U$ and $W$. Prove that if $T \in \text{End}(V)$ such that $U$ and $W$ are invariant under $T$, then the subspace spanned by $U$ and $W$ is invariant under $T$. 

    \begin{proof}
        Let the vector space $Z$ represent the subspace spanned by $U+W$.
        \[Z = \text{span}(\set{U+W})\]
        Meaning any vector $z\in Z$ is of the form $z = \alpha u + \beta w$ where $u$ and $w$ are vectors of $U$ and $W$ respectively with $\alpha$ and $\beta$ begin scalars. This gives us,
        \begin{align*}
            T(z) = T(\alpha u + \beta w) = \alpha T( u) + \beta T( w) 
        \end{align*}

        Recall though since $U$ and $W$ are invariant under $T$ we have that $T(u)\in U$ and $T(w) \in W$ and $Z$ is the span of $U+W$, therefore,
        \begin{align*}
            \alpha T( u) + \beta T( w)  \in Z
        \end{align*}

        Meaning $Z$ is invariant under $T$. $Z$ was defined to be the span of $U+W$. So we have that if $U$ and $W$ are invariant under $T$ then the subspace spanned by $U$ and $W$ is also invariant under $T$.
    \end{proof}
    
%------------------------------------------------------PROBLEM 8--------------------------------------------------------- 
    \item[$\textbf{[8]}$]
    Let $V$ be a $\mathbb{F}$-vector space with $E,F: V \rightarrow V$ projections. 
    \begin{itemize}

    \vspace{.3cm}
    \item[(a)]
    Prove that $\text{im}(E) = \text{im}(F)$ if and only if $EF = F$ and $FE = E$.
    \begin{proof}
        For the forward direction we will assume im$(E) =$ im$(F)$. This means for any vector $x\in V$ there exists some vector $y\in V$ such that $E(x) = F(y)$. 
        Now consider,
        \begin{align*}
            EF(y) &= EE(x) \\ 
            &= E(E(x)) && \text{Recall though all projections are idempotent} \\
            &= E(x) \\ 
            &= F(y) \\
            EF &= F
        \end{align*}
        Now consider,
        \begin{align*}
            FE(x) &= FF(y) \\
            &= F(F(y)) && \text{Recall though all projections are idempotent} \\
            &= F(y) \\
            &= E(x) \\
            FE &= E
        \end{align*}
        as desired. So now we have if im$(E) = \text{im}(F)$ implies $EF = F$ and $FE = E$
        Now for the reverse direction, we will assume $EF = F$ and $FE = E$. This means for any vector $x\in V$ we have, $EF(x) = F(x)$ and $FE(x) = E(x)$. 

        We know that for vector $x$, there exists some vector $y\in \text{im}(E)$ such that $E(x) = y$. Now consider the following,
        \begin{align*}
            E(x) &= y && \text{Recall our assumption $FE = E$} \\
            FE(x) &= y \\
            F(E(x)) &= y 
        \end{align*}
        Recall though $y$ was in the image of $E$ and now we can see that it is also in the image of $F$. Therefore $\text{im}(E) \subseteq \text{im}(F)$.

        We also know though that for a vector $x$, there exists some vector $y \in \text{im}(F)$ such that $F(x) = y$. Now consider the following,
        \begin{align*}
            F(x) &= y && \text{Recall our assupmtion $EF = F$} \\
            EF(x) &= y \\
            E(F(x)) &= y
        \end{align*}

        $y$ started in the image of $F$, but we can see that it is also in the image of $E$. This gives us that $\text{im}(F) \subseteq \text{im}(E)$

        Putting this all together we have, \[\text{im}(E) \subseteq \text{im}(F) \text{ and } \text{im}(F) \subseteq \text{im}(E) \rightarrow \text{im}(F) = \text{im}(E)\]
        as desired.
    \end{proof}

    \vspace{.3cm}
    \item[(b)]
    Prove that $\ker(E) = \ker(F)$ if and only $EF = E$ and $FE = F$ 
    \begin{proof}
        First we will go in the forward direction and assume $\ker(E) = \ker(F)$. This means whenever a vector $x\in V$ satisfies $E(x) = 0$ then it must also satisfy $F(x) = 0$. 

        We want to show that $EF =E$ and $FE = F$. First let's work with $EF = E$. If this equality were to hold that would mean for any vector $x$ in $V$ we would have,
        \[(E-EF(x) = 0)\]
        So let's assume that it doesn't hold, that would mean there exists some vector $y$ in $V$ such that,
        \[(E-EF)(y) \neq 0\]
        We see though such a $y$ would imply this about the kernel of $E$,
        \begin{align*}
            (E-EF)(y) &\neq 0 \\
            (E(y) - EF(y)) &\neq 0 \\
            E(y - F(y)) & \neq 0 
        \end{align*}
        It's that since $E(y - F(y)) \neq 0$ that means it is NOT in the kernel of $E$. Recall though our assumption was that $\ker(E) = \ker(F)$, that means this is also not in the kernel of $F$. But,
        \begin{align*}
            F(y - F(y)) &\neq 0 \\
            F(y) - F(F(y)) &\neq 0 && \text{Recall though every projection is idempotent} \\
            F(y) - F(y) &\neq 0 \\ 
            F(y) &\neq F(y)
        \end{align*}
        Which is a contradiction. Therefore if the kernel of the projection $E$ and $F$ are the same then $EF = E$. 

        We can take a similar look at $FE = F$ and see if this weren't true there would exist some vector $y$ in $V$ such that,
        \[(F-FE)(y) \neq 0\]
        Assuming this vector did indeed exist,
        \begin{align*}
            (F- FE)(y) &\neq 0 \\
            F(y) - FE(y) &\neq 0 \\
            F(y - E(y)) &\neq 0
        \end{align*}
        it would mean $(y- E(y))$ is not in the kernel of $E$. We see though,
        \begin{align*}
            E(y - E(y)) &\neq 0 \\
            E(y) -E(E(y)) &\neq 0 && \text{Projections are idempotent} \\
            E(y) - E(y) &\neq 0 \\
            E(y) &\neq E(y)
        \end{align*}
        which is again a contradiction. Therefore if the kernel of $E$ and $F$ are equal then $EF = E$ and $FE = F$

        Putting all this together: $\ker(F) = \ker(E)$ if and only if $EF = E$ and $FE = E$
    
    \end{proof}
    
    \end{itemize}
    
    \vspace{.5cm}
%------------------------------------------------------PROBLEM 9---------------------------------------------------------  
    \item[$\textbf{[9]}$]
    \begin{itemize}
    
    \item[(a)]
    Prove that if $E$ is a projection on a finite-dimensional $\mathbb{F}$-vector space, then there exists a basis $\mathcal{B}$ such that the matrix representative $[E]_\mathcal{B}$ has the following special form: $e_{ij} = 0$ if $i \neq j$ and $e_{ii} = 0$ or $1$ for all $i$ and $j$. 

    \begin{proof}
        First we will show why this exists. One important property about projections is that for any projection $E$ it must be idempotent, in other words it must satisfy the following $E^2 = E$

        If we recall how multiplication is defined between square matrices between matrix $A$ and $B$ to obtain $C$ we get the following,
        \begin{align*}
            C = \begin{pmatrix}
                c_{11} & c_{22} & \dots & c_{1n} \\
                c_{21} & c_{22} & \dots & c_{2n} \\
                \vdots & \vdots & & \vdots \\
                c_{n1} & c_{n2} & \dots & c_{nn}
            \end{pmatrix}
        \end{align*}
        where $c_{ij} = a_{i1}b_{ij} + a_{i2}b_{2j} + \dots + a_{in}b_{nj} = \sum_{k = 1}^n a_{ik}b_{kj}$ 

        Recall though in this situation $A = B = E$. So if all our non diagonal terms are non zero the only time we could possibly have a non zero value would be when we are multiplying elements along the diagonal. In other words  for $i = j$ $c_{ij} = a_{ij} b_{ji} = a_{ii}a_{ii}$. If $e_{ii} = 0$ then $c_{ii} = 0$ if $e_{ii} = 1$ then $c_{ii} = 1$. Meaning regardless if $e_{ii} = 0 $ or $1$. A projection such that the matrix representative has this special form will satisfy the requirement of being a projection. 

        Now if there did exists some other matrix representative not of this form we'd see it wouldn't satisfy $E^2 = E$. Since first consider if the diagonal could be something other than 0 or 1. We'd see it the requirement would fail due to $x^2 \neq x$ for any $x \neq 0,1$. Then the same reasoning applies to if non diagonal entries
    \end{proof}
    
    \vspace{.3cm}
    \item[(b)]
    An \textit{involution} is a linear transformation $U$ on a $\mathbb{F}$-vector space $V$ such that $U^2 = I$. Show that if $\text{char}(\mathbb{F}) \neq 2$, then the equation $U = 2E - I$ establishes a one-to-one correspondence between all projections $E$ and all involutions $U$.
    \begin{proof}
        Assuming we are not working in a field of characteristic 2. First let us begin with some involution $U$ we can obtain its respective projection $E$ through the following,
        \begin{align*}
            E = \dfrac{U + I}{2}
        \end{align*}
        We know this satisfies the property of begin a projection through the following. 
        \begin{align*}
            E^2 = \dfrac{U+I}{2}^2 &= \dfrac{U+I}{2}\dfrac{U+I}{2} \\
            &= \dfrac{U^2 + U + U + U^2}{4}  && \text{$U$ is an involution so,}\\
            &= \dfrac{2U + 2I}{4} \\
            &= \dfrac{2(U + I)}{2(2)} \\
            &= \dfrac{U + I}{2} = E
        \end{align*}
    

    Then we see for any projection we can obtain its respective involution through,
    \begin{align*}
        U = 2E - I
    \end{align*}
    We can see this indeed satisfies being an involution through the following.
    \begin{align*}
        U^2 &= (2E - I)^2 \\
        &= 4E^2 -2E -2E + I^2 && \text{Recall though $E$ is a projection so,}
        &= 4E - 4E + I \\
        &= 0 + I \\
        &= I
    \end{align*}
    We have thus showed the 1-1 correspondence due to being able to obtain any projections respective involution and vice versa. 
\end{proof}
    \vspace{.3cm}
    \item[(c)]
    Prove that the only eigenvalues of a projection are $0$ and $1$. Furthermore, prove that the only eigenvalues of an involution are $-1$ and $1$. (This does not require the vector space to be finite-dimensional.)
    
    \end{itemize}
    
    
%------------------------------------------------------PROBLEM 10--------------------------------------------------------
    \item[$\textbf{[10]}$]
    Find all the (complex) eigenvalues and eigenvectors of the following matrices over $\mathbb{C}$:
    \begin{equation*}
    \mathbf{A} = \begin{pmatrix}
    0 & 1 \\
    0 & 0
    \end{pmatrix}, \hspace{.3cm} \mathbf{B} = \begin{pmatrix}
    1 & 0 \\
    0 & i
    \end{pmatrix}, \hspace{.3cm} \mathbf{C} = \begin{pmatrix}
    1 & 1 \\
    0 & i
    \end{pmatrix}, \hspace{.3cm} \mathbf{D} = \begin{pmatrix}
    1 & 1 & 1 \\
    1 & 1 & 1 \\
    1 & 1 & 1
    \end{pmatrix}, \hspace{.3cm} \text{and} \hspace{.3cm} \mathbf{E} = \begin{pmatrix}
    1 & 1 & 1 \\
    0 & 1 & 1 \\
    0 & 0 & 1
    \end{pmatrix}
    \end{equation*}
    
    \end{itemize}

    \textbf{Solution:}

    \textbf{A)} Solving for the eigenvalues and eigenvectors of $A$, det$(A - \lambda I) = \lambda^2$. Solving for $\lambda^2 = 0$ we get $\lambda = 0$. 

    Now to get the corresponding eigenvector we get,
    \begin{align*}
        \begin{pmatrix}
            0 & 1 \\
            0 & 0
            \end{pmatrix} \begin{pmatrix}
                x \\ y
            \end{pmatrix} = \begin{pmatrix}
                0 \\ 0
            \end{pmatrix} \rightarrow \begin{pmatrix}x \\ y \end{pmatrix} = \begin{pmatrix}1 \\ 0 \end{pmatrix}
    \end{align*}

    Thus the eigenvalue for $A$ is 0 and its corresponding eigenvector is $\begin{pmatrix}1 \\ 0 \end{pmatrix}$

    \textbf{B)} Solving for the eigenvalues and eigenvectors of $B$, det$(B - \lambda I) = \lambda^2 -\lambda -\lambda i + i$. Solving for $\lambda$ we get, $\lambda = i,1$. 

    Now lets obtain the corresponding eigenvector for $\lambda_1 = i$,
    \begin{align*}
        \begin{pmatrix}
            1-i & 0 \\
            0 & 0
        \end{pmatrix} \begin{pmatrix}x\\ y \end{pmatrix} = \begin{pmatrix}0 \\ 0 \end{pmatrix} \rightarrow \begin{pmatrix}x\\ y\end{pmatrix} = \begin{pmatrix}0 \\ 1 \end{pmatrix}
    \end{align*}

    Now for $\lambda_2 = 1$,
    \begin{align*}
        \begin{pmatrix}
            0 & 0 \\
            0 & i-1
        \end{pmatrix} \begin{pmatrix}x\\ y \end{pmatrix} = \begin{pmatrix}0 \\ 0 \end{pmatrix} \rightarrow\begin{pmatrix}x\\ y\end{pmatrix} =  \begin{pmatrix}1\\ 0 \end{pmatrix}
    \end{align*}

    Thus we have for the eigenvalues $\lambda_1 = i$ and $\lambda_2 = 1$ the corresponding eigenvectors are $\begin{pmatrix}0 \\ 1 \end{pmatrix}$ and $\begin{pmatrix}1\\ 0 \end{pmatrix}$ respectively. 

    \textbf{C)} For $C$ we have det$(C -\lambda I) = i - \lambda - \lambda i + \lambda^2$. Solving for $\lambda$ we get the following eigenvalues: 1, $i$.

    Let's obtain the corresponding eigenvector for $\lambda_1 = i$,
    \begin{align*}
        \begin{pmatrix}
            1-i & 1 \\
            0 & 0
            \end{pmatrix} \begin{pmatrix}
                x \\ y 
            \end{pmatrix} = \begin{pmatrix}
                0 \\ 0
            \end{pmatrix} 
    \end{align*} 
    We get for the first equation $(1-i)x + y = 0$. We see if we set $x = (-1 - i)$ we get, $-1 + i -i -1 + y = -2 + y = 0$. Therefore $y = 2$. Thus the corresponding eigenvector is $\begin{pmatrix}
        -1 -i \\ 2
    \end{pmatrix}$
    \newcommand {\la} {{\lambda}}
    Now to obtain the corresponding eigenvector for $\lambda_2 = 1$,
    \begin{align*}
        \begin{pmatrix}
            0 & 1 \\
            0 & i-1
        \end{pmatrix} \begin{pmatrix}
                x \\ y 
        \end{pmatrix} = \begin{pmatrix}
                0 \\ 0
        \end{pmatrix} \rightarrow \begin{pmatrix}
            x \\ y
        \end{pmatrix} = \begin{pmatrix}
            1 \\ 0
        \end{pmatrix}
    \end{align*}

    Thus our eigenvector for eigenvalues $\lambda_1 = i$ and $\lambda_2 = 1$ are $\begin{pmatrix}
        -1 -i \\ 2
    \end{pmatrix}$ and $\begin{pmatrix}
        1 \\ 0
    \end{pmatrix}$ respectively.

    \textbf{D)} For $D$ we have, det$(D - \lambda I) = -\lambda^3 + 3\lambda^2 = -\lambda^2(\lambda - 3)$. Solving for $\lambda$ we get $\lambda = 0,3$.

    Now solving for the corresponding eigenvector for $\la_1 = 3$, (Some steps I'm skipping over since it would be a lot of matrices to type out)
    \begin{align*}
        \begin{pmatrix}
            -2 & 1 & 1 \\
            1 & -2 & 1 \\
            1 & 1 & -2
        \end{pmatrix} \rightarrow \begin{pmatrix}
            1 & -\frac{1}{2} & -\frac{1}{2} \\
            1 & -2 & 1 \\ 
            1 & 1 & -2
        \end{pmatrix} \rightarrow \dots \rightarrow \begin{pmatrix}
            1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 0
        \end{pmatrix}\begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} = \begin{pmatrix}
            0 \\ 0 \\ 0
        \end{pmatrix}
    \end{align*}
    This gives us that $x_1 = x_2$ by the first row, and $x_2 = x_3$ by the second row. Thus our corresponding eigenvector is, $\begin{pmatrix}
        1 \\ 1 \\ 1
    \end{pmatrix}$

    Now for the corresponding eigenvector for $\la_2 = 0 $,
    \begin{align*}
        \begin{pmatrix}
            1& 1 & 1 \\
            1 & 1 & 1 \\
            1 & 1 & 1
        \end{pmatrix} \rightarrow \begin{pmatrix}
            1 & 1 & 1 \\ 1 & 1 & 1 \\ 0 & 0 & 0 
        \end{pmatrix} \rightarrow \begin{pmatrix}
            1 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0
        \end{pmatrix} \begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix}= \begin{pmatrix}
            0 \\ 0 \\ 0
        \end{pmatrix}
    \end{align*}

    We see the corresponding eigenvectors to be $\begin{pmatrix}
        -1 \\ 1 \\0
    \end{pmatrix}$ and $\begin{pmatrix}
        -1 \\ 0 \\1
    \end{pmatrix}$

    Thus for the eigenvalue $\la_1 = 3$ the corresponding eigenvector is $\begin{pmatrix}
        1 \\ 1 \\ 1
    \end{pmatrix}$ and for $\la_2 = 0$ the corresponding eigenvectors are $\begin{pmatrix}
        -1 \\ 1 \\0
    \end{pmatrix}$ and $\begin{pmatrix}
        -1 \\ 0 \\1
    \end{pmatrix}$

    \textbf{D)} For $D$ we have det$(D - \lambda I) = -\la^3 + 3\la^2 - 3\la +1 = -(\la -1)^3$. Solving for $\la$ we get $\la = 1$.

    Now solving for the corresponding eigenvector we get,
    \begin{align*}
        \begin{pmatrix}
            0 & 1 & 1 \\
            0 & 0 & 1 \\
            0 & 0 & 0
            \end{pmatrix} \rightarrow \begin{pmatrix}
                0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 &0 
            \end{pmatrix} \begin{pmatrix}
                x \\ y \\ z
            \end{pmatrix} = \begin{pmatrix}
                0 \\ 0\\ 0
            \end{pmatrix} \rightarrow \begin{pmatrix}
                x \\ y \\ z 
            \end{pmatrix} = \begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix}
    \end{align*}

    Thus we see the corresponding eigenvector for $\la = 1$ is $\begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix}$
\end{document}